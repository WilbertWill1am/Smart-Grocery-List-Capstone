{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Grocery Price Prediction Documentation\n",
    "\n",
    "This documentation provides an overview of the Smart Grocery Price Prediction code. The code uses machine learning techniques to predict grocery prices based on various features, such as market location, category, commodity, and geographical coordinates. It also provides recommendations for the nearest market locations with the lowest prices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To run the code successfully, you need to have the following dependencies installed:\n",
    "\n",
    "- pandas\n",
    "- tensorflow\n",
    "- numpy\n",
    "- matplotlib\n",
    "- geopy\n",
    "\n",
    "After you installed the dependencies, now import the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, concatenate, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from geopy.distance import geodesic\n",
    "from tensorflow import keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Overview\n",
    "\n",
    "The code is divided into several sections:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Loading and Preprocessing**: In this section, the dataset is loaded from a CSV file and preprocessed. Missing values are dropped, latitude and longitude columns are formatted and converted to float, and scaling is applied to the numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a DataFrame\n",
    "dataset = pd.read_csv('Data_Harga1.csv', delimiter=';')\n",
    "\n",
    "# Drop rows with missing values\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Preprocess latitude and longitude columns\n",
    "dataset['latitude'] = dataset['latitude'].str.replace('.', '').str.replace('..', '.')\n",
    "dataset['longitude'] = dataset['longitude'].str.replace('.', '').str.replace('..', '.')\n",
    "\n",
    "# Convert latitude and longitude columns to float\n",
    "dataset['latitude'] = dataset['latitude'].astype(float)\n",
    "dataset['longitude'] = dataset['longitude'].astype(float)\n",
    "\n",
    "# Apply the formatting to latitude and longitude columns\n",
    "dataset['latitude'] = dataset['latitude'] / 1000000\n",
    "dataset['longitude'] = dataset['longitude'] / 1000000\n",
    "\n",
    "print(dataset.head)\n",
    "print(dataset.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Data Preparation**: The dataset is split into features (X) and the target variable (y). Text data is tokenized and padded using the Tokenizer and pad_sequences functions from Keras. The combined features are scaled using StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the relevant columns for features and target variable\n",
    "features = ['pasar', 'category', 'commodity', 'unit', 'latitude', 'longitude']\n",
    "target = 'price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features and target variable\n",
    "X = dataset[features]\n",
    "y = dataset[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform label encoding on the categorical features\n",
    "label_encoder = LabelEncoder()\n",
    "for feature in ['pasar', 'category', 'commodity', 'unit']:\n",
    "    X[feature] = label_encoder.fit_transform(X[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data to token sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X['pasar'].astype(str))  # Convert values to strings\n",
    "X_pasar = tokenizer.texts_to_sequences(X['pasar'].astype(str))\n",
    "X_pasar = pad_sequences(X_pasar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the processed data\n",
    "X_combined = np.concatenate((X_scaled, X_pasar), axis=1)\n",
    "\n",
    "# Shuffle and split the data\n",
    "X_combined_train, X_combined_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Separate the combined data into text and numeric inputs\n",
    "X_pasar_train = X_combined_train[:, -X_pasar.shape[1]:]\n",
    "X_train_train = X_combined_train[:, :-X_pasar.shape[1]]\n",
    "X_pasar_test = X_combined_test[:, -X_pasar.shape[1]:]\n",
    "X_train_test = X_combined_test[:, :-X_pasar.shape[1]]\n",
    "\n",
    "# Print the shapes of the data splits\n",
    "print(\"X_train_train shape:\", X_train_train.shape)\n",
    "print(\"X_train_test shape:\", X_train_test.shape)\n",
    "print(\"X_pasar_train shape:\", X_pasar_train.shape)\n",
    "print(\"X_pasar_test shape:\", X_pasar_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the maximum length\n",
    "max_length = X_pasar.shape[1]\n",
    "\n",
    "print(max_length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Model Architecture**: The code defines a model architecture using the Keras functional API. The architecture consists of an embedding layer for text input, a reshape layer for numeric input, concatenation of both inputs, an LSTM layer, and a dense output layer for price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "embedding_dim = 100  # Adjust the embedding dimension as needed\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Text input\n",
    "text_input = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)(text_input)\n",
    "\n",
    "# Numeric input\n",
    "numeric_input_shape = (X_train_train.shape[1],)\n",
    "numeric_input = Input(shape=numeric_input_shape)\n",
    "reshaped_numeric_input = Reshape((1, numeric_input_shape[0]))(numeric_input)\n",
    "\n",
    "# Concatenate text and numeric inputs\n",
    "concatenated = concatenate([embedding_layer, reshaped_numeric_input], axis=-1)\n",
    "\n",
    "# LSTM layer\n",
    "lstm_layer = LSTM(128)(concatenated)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='linear')(lstm_layer)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[text_input, numeric_input], outputs=output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Model Training**: The model is compiled with the mean absolute error loss function and the Adam optimizer. It is then trained on the training data with 500 epochs and a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "history = model.fit([X_pasar_train, X_train_train], y_train, epochs=500, batch_size=32, validation_data=([X_pasar_test, X_train_test], y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Model Evaluation**: The trained model is evaluated on the test data using the mean absolute error as the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss = model.evaluate([X_pasar_test, X_train_test], y_test)\n",
    "\n",
    "# Print the evaluation result\n",
    "print(\"Test Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and validation loss values from the history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot the training and validation loss\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "plt.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'g-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **User Input and Price Prediction**: User inputs for market location, geographical coordinates, category, and commodity are provided. The inputs are preprocessed, encoded, and combined with numeric features. The model predicts the grocery price based on the user inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinsi = \"JAWA TENGAH\"\n",
    "kabupaten = \"KOTA SEMARANG\"\n",
    "latitude = -7.058040\n",
    "longitude = 110.445680\n",
    "category = \"meat, fish and eggs\"\n",
    "commodity = \"Eggs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess user input\n",
    "user_pasar = tokenizer.texts_to_sequences([kabupaten])  # Tokenize 'pasar'\n",
    "user_pasar = pad_sequences(user_pasar, maxlen=max_length)  # Pad the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform the categorical features\n",
    "user_provinsi_encoded = onehot_encoder.fit_transform([[provinsi]])\n",
    "user_category_encoded = onehot_encoder.transform([[category]])\n",
    "user_commodity_encoded = onehot_encoder.transform([[commodity]])\n",
    "user_kabupaten_encoded = onehot_encoder.transform([[kabupaten]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the numeric and categorical features\n",
    "user_numeric_features = np.array([[latitude, longitude]])  # Reshape to (1, 2)\n",
    "user_features = np.concatenate((user_numeric_features, user_provinsi_encoded, user_kabupaten_encoded, user_category_encoded, user_commodity_encoded), axis=1)\n",
    "\n",
    "# Take only the first three elements from user_features\n",
    "user_features = user_features[:, :6]\n",
    "\n",
    "# Make predictions\n",
    "predicted_price = model.predict([user_pasar, user_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the predicted prices in ascending order\n",
    "sorted_indices = np.argsort(predicted_price.flatten())\n",
    "sorted_prices = predicted_price[sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top five market locations with the lowest predicted prices\n",
    "recommended_locations = dataset.iloc[sorted_indices[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data based on user input\n",
    "filtered_data = dataset[(dataset['provinsi'] == provinsi) & (dataset['kabupaten'] == kabupaten) & (dataset['category'] == category) & (dataset['commodity'] == commodity)].copy()\n",
    "print(\"Filtered Data Length:\", len(filtered_data))  # Add this line to check the length of filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame to store recommended locations with distances\n",
    "recommended_locations = pd.DataFrame(columns=['pasar', 'price', 'latitude', 'longitude', 'distance'])\n",
    "\n",
    "# Create a set to store the visited markets\n",
    "visited_markets = set()\n",
    "\n",
    "# Calculate distances and add rows to the recommended_locations DataFrame\n",
    "for i, row in filtered_data.iterrows():\n",
    "    market = row['pasar']\n",
    "    if market not in visited_markets:\n",
    "        distance = geodesic((latitude, longitude), (row['latitude'], row['longitude'])).km\n",
    "        recommended_locations.loc[i] = [market, row['price'], row['latitude'], row['longitude'], distance]\n",
    "        visited_markets.add(market)\n",
    "\n",
    "        # Break the loop if 5 unique markets have been added\n",
    "        if len(recommended_locations) == 5:\n",
    "            break\n",
    "\n",
    "# Sort the recommendations by price and distance\n",
    "sorted_recommendations = recommended_locations.sort_values(['price', 'distance'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Market Location Recommendations**: Based on the user inputs, the code filters the dataset to find matching market locations. It calculates the distances between the user's location and each market location using the geodesic function from the geopy library. The code recommends the top five market locations with the lowest prices and closest distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the recommendations\n",
    "print(\"User input: \")\n",
    "print(provinsi)\n",
    "print(kabupaten)\n",
    "print(latitude) \n",
    "print(longitude)\n",
    "print(category)\n",
    "print(commodity)\n",
    "print()\n",
    "\n",
    "print(\"Top 5 Recommended Market Locations:\")\n",
    "for i, row in sorted_recommendations.head(5).iterrows():\n",
    "    print(f\"Market: {row['pasar']}\")\n",
    "    print(f\"Price: {row['price']}\")\n",
    "    print(f\"Distance: {row['distance']} km\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **Save The Model**: Save the model using h5 and pickle format. The model will saved as \"Store_Recommendation.h5\" and \"model.pickle\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Store_Recommendation.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "To run the code, follow these steps:\n",
    "\n",
    "1. Install the necessary dependencies mentioned in the Prerequisites section.\n",
    "\n",
    "2. Make sure you have the 'Data_Harga1.csv' dataset in the same directory as the code file.\n",
    "\n",
    "3. Run the code file using a Python interpreter.\n",
    "\n",
    "4. The code will prompt for user inputs such as the province, district, latitude, longitude, category, and commodity. Provide the required information and press Enter.\n",
    "\n",
    "5. The code will display the predicted grocery price based on the user inputs.\n",
    "\n",
    "6. It will also show the top five recommended market locations based on price and distance.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The Smart Grocery Price Prediction code demonstrates the use of machine learning techniques to predict grocery prices and provide recommendations for market locations. It utilizes text and numeric inputs, employs an LSTM-based model architecture, and incorporates geographic distance calculations. By running this code, users can predict grocery prices and find nearby market locations with the best prices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
