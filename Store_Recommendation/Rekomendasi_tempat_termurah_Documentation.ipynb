{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Grocery Price Prediction Documentation\n",
    "\n",
    "This documentation provides an overview of the Smart Grocery Price Prediction code. The code uses machine learning techniques to predict grocery prices based on various features, such as market location, category, commodity, and geographical coordinates. It also provides recommendations for the nearest market locations with the lowest prices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To run the code successfully, you need to have the following dependencies installed:\n",
    "\n",
    "- pandas\n",
    "- tensorflow\n",
    "- numpy\n",
    "- matplotlib\n",
    "- geopy\n",
    "\n",
    "After you installed the dependencies, now import the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, concatenate, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from geopy.distance import geodesic\n",
    "from tensorflow import keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Overview\n",
    "\n",
    "The code is divided into several sections:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Loading and Preprocessing**: In this section, the dataset is loaded from a CSV file and preprocessed. Missing values are dropped, latitude and longitude columns are formatted and converted to float, and scaling is applied to the numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of              provinsi     kabupaten                pasar  latitude  longitude   \n",
      "0                ACEH    ACEH BARAT         Pasar Lapang  4.173922  96.141854  \\\n",
      "1                ACEH    ACEH BARAT         Pasar Lapang  4.173922  96.141854   \n",
      "2                ACEH    ACEH BARAT         Pasar Lapang  4.173922  96.141854   \n",
      "3                ACEH    ACEH BARAT         Pasar Lapang  4.173922  96.141854   \n",
      "4                ACEH    ACEH BARAT         Pasar Lapang  4.173922  96.141854   \n",
      "...               ...           ...                  ...       ...        ...   \n",
      "53557  SUMATERA UTARA  KOTA SIBOLGA  Pasar Nauli Sibolga  1.738378   9.878446   \n",
      "53558  SUMATERA UTARA  KOTA SIBOLGA  Pasar Nauli Sibolga  1.738378   9.878446   \n",
      "53559  SUMATERA UTARA  KOTA SIBOLGA  Pasar Nauli Sibolga  1.738378   9.878446   \n",
      "53560  SUMATERA UTARA  KOTA SIBOLGA  Pasar Nauli Sibolga  1.738378   9.878446   \n",
      "53561  SUMATERA UTARA  KOTA SIBOLGA  Pasar Nauli Sibolga  1.738378   9.878446   \n",
      "\n",
      "                    category                  commodity unit     price  \n",
      "0         cereals and tubers        Rice (high quality)   KG  11117.86  \n",
      "1         cereals and tubers         Rice (low quality)   KG   9388.10  \n",
      "2         cereals and tubers      Rice (medium quality)   KG  10090.48  \n",
      "3        meat, fish and eggs             Meat (chicken)   KG  30476.19  \n",
      "4        meat, fish and eggs    Meat (chicken, broiler)   KG  30476.19  \n",
      "...                      ...                        ...  ...       ...  \n",
      "53557  vegetables and fruits  Chili (bird's eye, green)   KG  31119.05  \n",
      "53558  vegetables and fruits                Chili (red)   KG  31238.10  \n",
      "53559  vegetables and fruits         Chili (red, curly)   KG  31238.10  \n",
      "53560  vegetables and fruits            Garlic (medium)   KG  29904.76  \n",
      "53561  vegetables and fruits   Onions (shallot, medium)   KG  37928.57  \n",
      "\n",
      "[53562 rows x 9 columns]>\n",
      "Index(['provinsi', 'kabupaten', 'pasar', 'latitude', 'longitude', 'category',\n",
      "       'commodity', 'unit', 'price'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset into a DataFrame\n",
    "dataset = pd.read_csv('Data_Harga1.csv', delimiter=';')\n",
    "\n",
    "# Drop rows with missing values\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Preprocess latitude and longitude columns\n",
    "dataset['latitude'] = dataset['latitude'].str.replace('.', '').str.replace('..', '.')\n",
    "dataset['longitude'] = dataset['longitude'].str.replace('.', '').str.replace('..', '.')\n",
    "\n",
    "# Convert latitude and longitude columns to float\n",
    "dataset['latitude'] = dataset['latitude'].astype(float)\n",
    "dataset['longitude'] = dataset['longitude'].astype(float)\n",
    "\n",
    "# Apply the formatting to latitude and longitude columns\n",
    "dataset['latitude'] = dataset['latitude'] / 1000000\n",
    "dataset['longitude'] = dataset['longitude'] / 1000000\n",
    "\n",
    "print(dataset.head)\n",
    "print(dataset.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Data Preparation**: The dataset is split into features (X) and the target variable (y). Text data is tokenized and padded using the Tokenizer and pad_sequences functions from Keras. The combined features are scaled using StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the relevant columns for features and target variable\n",
    "features = ['pasar', 'category', 'commodity', 'unit', 'latitude', 'longitude']\n",
    "target = 'price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features and target variable\n",
    "X = dataset[features]\n",
    "y = dataset[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wilbert\\AppData\\Local\\Temp\\ipykernel_14260\\432410839.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[feature] = label_encoder.fit_transform(X[feature])\n",
      "C:\\Users\\Wilbert\\AppData\\Local\\Temp\\ipykernel_14260\\432410839.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[feature] = label_encoder.fit_transform(X[feature])\n",
      "C:\\Users\\Wilbert\\AppData\\Local\\Temp\\ipykernel_14260\\432410839.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[feature] = label_encoder.fit_transform(X[feature])\n",
      "C:\\Users\\Wilbert\\AppData\\Local\\Temp\\ipykernel_14260\\432410839.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[feature] = label_encoder.fit_transform(X[feature])\n"
     ]
    }
   ],
   "source": [
    "# Perform label encoding on the categorical features\n",
    "label_encoder = LabelEncoder()\n",
    "for feature in ['pasar', 'category', 'commodity', 'unit']:\n",
    "    X[feature] = label_encoder.fit_transform(X[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data to token sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X['pasar'].astype(str))  # Convert values to strings\n",
    "X_pasar = tokenizer.texts_to_sequences(X['pasar'].astype(str))\n",
    "X_pasar = pad_sequences(X_pasar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_train shape: (37493, 6)\n",
      "X_train_test shape: (16069, 6)\n",
      "X_pasar_train shape: (37493, 1)\n",
      "X_pasar_test shape: (16069, 1)\n",
      "y_train shape: (37493,)\n",
      "y_test shape: (16069,)\n"
     ]
    }
   ],
   "source": [
    "# Combine the processed data\n",
    "X_combined = np.concatenate((X_scaled, X_pasar), axis=1)\n",
    "\n",
    "# Shuffle and split the data\n",
    "X_combined_train, X_combined_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Separate the combined data into text and numeric inputs\n",
    "X_pasar_train = X_combined_train[:, -X_pasar.shape[1]:]\n",
    "X_train_train = X_combined_train[:, :-X_pasar.shape[1]]\n",
    "X_pasar_test = X_combined_test[:, -X_pasar.shape[1]:]\n",
    "X_train_test = X_combined_test[:, :-X_pasar.shape[1]]\n",
    "\n",
    "# Print the shapes of the data splits\n",
    "print(\"X_train_train shape:\", X_train_train.shape)\n",
    "print(\"X_train_test shape:\", X_train_test.shape)\n",
    "print(\"X_pasar_train shape:\", X_pasar_train.shape)\n",
    "print(\"X_pasar_test shape:\", X_pasar_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Calculate the maximum length\n",
    "max_length = X_pasar.shape[1]\n",
    "\n",
    "print(max_length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Model Architecture**: The code defines a model architecture using the Keras functional API. The architecture consists of an embedding layer for text input, a reshape layer for numeric input, concatenation of both inputs, an LSTM layer, and a dense output layer for price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "embedding_dim = 100  # Adjust the embedding dimension as needed\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Text input\n",
    "text_input = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)(text_input)\n",
    "\n",
    "# Numeric input\n",
    "numeric_input_shape = (X_train_train.shape[1],)\n",
    "numeric_input = Input(shape=numeric_input_shape)\n",
    "reshaped_numeric_input = Reshape((1, numeric_input_shape[0]))(numeric_input)\n",
    "\n",
    "# Concatenate text and numeric inputs\n",
    "concatenated = concatenate([embedding_layer, reshaped_numeric_input], axis=-1)\n",
    "\n",
    "# LSTM layer\n",
    "lstm_layer = LSTM(128)(concatenated)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='linear')(lstm_layer)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[text_input, numeric_input], outputs=output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Model Training**: The model is compiled with the mean absolute error loss function and the Adam optimizer. It is then trained on the training data with 500 epochs and a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1172/1172 [==============================] - 10s 6ms/step - loss: 39882.1016 - val_loss: 40392.2734\n",
      "Epoch 2/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 39755.8047 - val_loss: 40274.4531\n",
      "Epoch 3/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 39639.1289 - val_loss: 40158.4102\n",
      "Epoch 4/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 39523.3594 - val_loss: 40042.8164\n",
      "Epoch 5/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 39407.7891 - val_loss: 39927.3438\n",
      "Epoch 6/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 39292.3555 - val_loss: 39811.9219\n",
      "Epoch 7/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 39176.9258 - val_loss: 39696.4609\n",
      "Epoch 8/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 39061.4688 - val_loss: 39580.9844\n",
      "Epoch 9/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 38945.9766 - val_loss: 39465.5586\n",
      "Epoch 10/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 38830.5547 - val_loss: 39350.0625\n",
      "Epoch 11/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 38715.0898 - val_loss: 39234.5859\n",
      "Epoch 12/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 38599.5547 - val_loss: 39119.1133\n",
      "Epoch 13/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 38484.1016 - val_loss: 39003.6719\n",
      "Epoch 14/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 38368.6719 - val_loss: 38888.2578\n",
      "Epoch 15/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 38253.3398 - val_loss: 38772.8906\n",
      "Epoch 16/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 38137.9375 - val_loss: 38657.5625\n",
      "Epoch 17/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 38022.6094 - val_loss: 38542.1797\n",
      "Epoch 18/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 37907.2461 - val_loss: 38426.8008\n",
      "Epoch 19/500\n",
      "1172/1172 [==============================] - 6s 6ms/step - loss: 37791.8555 - val_loss: 38311.4609\n",
      "Epoch 20/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 37676.5234 - val_loss: 38196.1055\n",
      "Epoch 21/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 37561.1562 - val_loss: 38080.7227\n",
      "Epoch 22/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 37445.7578 - val_loss: 37965.3789\n",
      "Epoch 23/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 37330.4219 - val_loss: 37850.0078\n",
      "Epoch 24/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 37215.0625 - val_loss: 37734.6523\n",
      "Epoch 25/500\n",
      "1172/1172 [==============================] - 6s 6ms/step - loss: 37099.7031 - val_loss: 37619.3086\n",
      "Epoch 26/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 36984.3125 - val_loss: 37503.9336\n",
      "Epoch 27/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 36869.0039 - val_loss: 37388.5703\n",
      "Epoch 28/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 36753.5820 - val_loss: 37273.2227\n",
      "Epoch 29/500\n",
      "1172/1172 [==============================] - 8s 7ms/step - loss: 36638.2734 - val_loss: 37157.8594\n",
      "Epoch 30/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 36522.9336 - val_loss: 37042.4844\n",
      "Epoch 31/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 36407.5742 - val_loss: 36927.1367\n",
      "Epoch 32/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 36292.1992 - val_loss: 36811.7812\n",
      "Epoch 33/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 36176.8125 - val_loss: 36696.4062\n",
      "Epoch 34/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 36061.4414 - val_loss: 36581.0547\n",
      "Epoch 35/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 35946.1172 - val_loss: 36465.6992\n",
      "Epoch 36/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 35830.7383 - val_loss: 36350.3398\n",
      "Epoch 37/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 35715.3906 - val_loss: 36234.9688\n",
      "Epoch 38/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 35600.0352 - val_loss: 36119.6328\n",
      "Epoch 39/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 35484.6445 - val_loss: 36004.2578\n",
      "Epoch 40/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 35369.2734 - val_loss: 35888.8906\n",
      "Epoch 41/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 35253.9414 - val_loss: 35773.5508\n",
      "Epoch 42/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 35138.5742 - val_loss: 35658.1797\n",
      "Epoch 43/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 35023.1836 - val_loss: 35542.8086\n",
      "Epoch 44/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 34907.8750 - val_loss: 35427.4609\n",
      "Epoch 45/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 34792.5273 - val_loss: 35312.1055\n",
      "Epoch 46/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 34677.1797 - val_loss: 35196.7266\n",
      "Epoch 47/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 34561.7812 - val_loss: 35081.3789\n",
      "Epoch 48/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 34446.4141 - val_loss: 34966.0039\n",
      "Epoch 49/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 34331.0781 - val_loss: 34850.6680\n",
      "Epoch 50/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 34215.6992 - val_loss: 34735.2852\n",
      "Epoch 51/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 34100.3242 - val_loss: 34619.9258\n",
      "Epoch 52/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 33984.9844 - val_loss: 34504.5859\n",
      "Epoch 53/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 33869.5977 - val_loss: 34389.2070\n",
      "Epoch 54/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 33754.2734 - val_loss: 34273.8516\n",
      "Epoch 55/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 33638.9062 - val_loss: 34158.5078\n",
      "Epoch 56/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 33523.5156 - val_loss: 34043.1289\n",
      "Epoch 57/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 33408.1758 - val_loss: 33927.7773\n",
      "Epoch 58/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 33292.8086 - val_loss: 33812.4102\n",
      "Epoch 59/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 33177.5000 - val_loss: 33697.0547\n",
      "Epoch 60/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 33062.0742 - val_loss: 33581.7031\n",
      "Epoch 61/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 32946.7500 - val_loss: 33466.3320\n",
      "Epoch 62/500\n",
      "1172/1172 [==============================] - 6s 6ms/step - loss: 32831.4141 - val_loss: 33350.9688\n",
      "Epoch 63/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 32716.0391 - val_loss: 33235.6289\n",
      "Epoch 64/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 32600.7188 - val_loss: 33120.2500\n",
      "Epoch 65/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 32485.3066 - val_loss: 33004.8984\n",
      "Epoch 66/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 32369.9883 - val_loss: 32889.5859\n",
      "Epoch 67/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 32254.6387 - val_loss: 32774.2383\n",
      "Epoch 68/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 32139.2793 - val_loss: 32658.9160\n",
      "Epoch 69/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 32023.9590 - val_loss: 32543.6797\n",
      "Epoch 70/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 31908.6699 - val_loss: 32428.4277\n",
      "Epoch 71/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 31793.3867 - val_loss: 32313.1953\n",
      "Epoch 72/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 31678.2598 - val_loss: 32198.1211\n",
      "Epoch 73/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 31563.1309 - val_loss: 32083.0312\n",
      "Epoch 74/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 31448.0605 - val_loss: 31968.0234\n",
      "Epoch 75/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 31333.2168 - val_loss: 31853.1738\n",
      "Epoch 76/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 31218.4434 - val_loss: 31738.3145\n",
      "Epoch 77/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 31103.6895 - val_loss: 31623.4570\n",
      "Epoch 78/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 30988.9414 - val_loss: 31508.6777\n",
      "Epoch 79/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 30874.4961 - val_loss: 31394.1680\n",
      "Epoch 80/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 30760.2305 - val_loss: 31279.7871\n",
      "Epoch 81/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 30646.2207 - val_loss: 31165.7793\n",
      "Epoch 82/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 30532.4766 - val_loss: 31052.1504\n",
      "Epoch 83/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 30419.1719 - val_loss: 30939.0898\n",
      "Epoch 84/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 30306.1992 - val_loss: 30826.2461\n",
      "Epoch 85/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 30193.7539 - val_loss: 30714.1133\n",
      "Epoch 86/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 30082.1055 - val_loss: 30602.3711\n",
      "Epoch 87/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 29971.1250 - val_loss: 30491.6934\n",
      "Epoch 88/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 29861.2637 - val_loss: 30381.7715\n",
      "Epoch 89/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 29751.8633 - val_loss: 30272.4590\n",
      "Epoch 90/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 29643.1758 - val_loss: 30163.6738\n",
      "Epoch 91/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 29534.9492 - val_loss: 30055.5566\n",
      "Epoch 92/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 29427.7441 - val_loss: 29948.5918\n",
      "Epoch 93/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 29321.3672 - val_loss: 29842.4766\n",
      "Epoch 94/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 29216.0332 - val_loss: 29737.6094\n",
      "Epoch 95/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 29111.5898 - val_loss: 29633.3340\n",
      "Epoch 96/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 29008.3789 - val_loss: 29530.7715\n",
      "Epoch 97/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 28906.4375 - val_loss: 29428.7520\n",
      "Epoch 98/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 28805.2090 - val_loss: 29327.4531\n",
      "Epoch 99/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 28704.8828 - val_loss: 29226.8535\n",
      "Epoch 100/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 28605.5566 - val_loss: 29127.5918\n",
      "Epoch 101/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 28507.6523 - val_loss: 29029.3633\n",
      "Epoch 102/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 28410.6992 - val_loss: 28932.1016\n",
      "Epoch 103/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 28314.8496 - val_loss: 28835.6914\n",
      "Epoch 104/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 28219.8594 - val_loss: 28740.4160\n",
      "Epoch 105/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 28126.4297 - val_loss: 28646.9062\n",
      "Epoch 106/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 28033.8496 - val_loss: 28553.9004\n",
      "Epoch 107/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 27942.0117 - val_loss: 28461.8555\n",
      "Epoch 108/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 27851.0410 - val_loss: 28370.4824\n",
      "Epoch 109/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 27761.0938 - val_loss: 28280.4160\n",
      "Epoch 110/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 27672.4219 - val_loss: 28191.2773\n",
      "Epoch 111/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 27584.4395 - val_loss: 28102.7832\n",
      "Epoch 112/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 27497.3379 - val_loss: 28015.1426\n",
      "Epoch 113/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 27410.7578 - val_loss: 27928.4590\n",
      "Epoch 114/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 27326.0020 - val_loss: 27843.3516\n",
      "Epoch 115/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 27242.0703 - val_loss: 27758.7031\n",
      "Epoch 116/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 27158.9316 - val_loss: 27675.0625\n",
      "Epoch 117/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 27076.6250 - val_loss: 27592.1191\n",
      "Epoch 118/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 26995.4570 - val_loss: 27510.9863\n",
      "Epoch 119/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 26915.7031 - val_loss: 27430.9102\n",
      "Epoch 120/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 26837.0801 - val_loss: 27351.9707\n",
      "Epoch 121/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 26759.5586 - val_loss: 27274.0664\n",
      "Epoch 122/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 26683.4824 - val_loss: 27199.5996\n",
      "Epoch 123/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 26612.6406 - val_loss: 27128.4512\n",
      "Epoch 124/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 26542.5664 - val_loss: 27057.5176\n",
      "Epoch 125/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 26473.6387 - val_loss: 26988.1543\n",
      "Epoch 126/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 26405.5723 - val_loss: 26919.1426\n",
      "Epoch 127/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 26339.1934 - val_loss: 26853.1562\n",
      "Epoch 128/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 26274.7188 - val_loss: 26787.5684\n",
      "Epoch 129/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 26210.7500 - val_loss: 26722.9648\n",
      "Epoch 130/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 26148.0371 - val_loss: 26659.0137\n",
      "Epoch 131/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 26086.1582 - val_loss: 26597.7012\n",
      "Epoch 132/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 26029.0352 - val_loss: 26539.6855\n",
      "Epoch 133/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 25972.4199 - val_loss: 26481.2871\n",
      "Epoch 134/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 25915.9238 - val_loss: 26423.5273\n",
      "Epoch 135/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 25860.1387 - val_loss: 26366.3262\n",
      "Epoch 136/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 25805.5605 - val_loss: 26311.2441\n",
      "Epoch 137/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 25752.4551 - val_loss: 26256.4434\n",
      "Epoch 138/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 25699.4648 - val_loss: 26202.4121\n",
      "Epoch 139/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 25647.1621 - val_loss: 26148.6836\n",
      "Epoch 140/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 25595.2031 - val_loss: 26096.0039\n",
      "Epoch 141/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 25545.3301 - val_loss: 26045.6094\n",
      "Epoch 142/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 25496.1191 - val_loss: 25994.9492\n",
      "Epoch 143/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 25447.0195 - val_loss: 25944.5742\n",
      "Epoch 144/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 25398.2402 - val_loss: 25894.3418\n",
      "Epoch 145/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 25349.7637 - val_loss: 25844.8848\n",
      "Epoch 146/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 25302.2891 - val_loss: 25796.0547\n",
      "Epoch 147/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 25255.0000 - val_loss: 25747.2402\n",
      "Epoch 148/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 25208.0879 - val_loss: 25698.8535\n",
      "Epoch 149/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 25161.4082 - val_loss: 25650.6328\n",
      "Epoch 150/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 25115.5449 - val_loss: 25603.9492\n",
      "Epoch 151/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 25070.4180 - val_loss: 25557.3613\n",
      "Epoch 152/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 25025.4180 - val_loss: 25510.9160\n",
      "Epoch 153/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 24980.7949 - val_loss: 25464.9824\n",
      "Epoch 154/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 24936.6113 - val_loss: 25419.3633\n",
      "Epoch 155/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 24892.9219 - val_loss: 25374.0781\n",
      "Epoch 156/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 24849.3633 - val_loss: 25329.1641\n",
      "Epoch 157/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 24806.3262 - val_loss: 25284.7949\n",
      "Epoch 158/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 24763.5430 - val_loss: 25240.6699\n",
      "Epoch 159/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 24721.1387 - val_loss: 25197.3828\n",
      "Epoch 160/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 24679.9238 - val_loss: 25154.8086\n",
      "Epoch 161/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 24639.0312 - val_loss: 25112.5020\n",
      "Epoch 162/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 24598.7305 - val_loss: 25070.8457\n",
      "Epoch 163/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 24558.6426 - val_loss: 25029.2910\n",
      "Epoch 164/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 24518.9727 - val_loss: 24988.5801\n",
      "Epoch 165/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 24480.0000 - val_loss: 24948.3359\n",
      "Epoch 166/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 24441.2852 - val_loss: 24908.2559\n",
      "Epoch 167/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 24402.9355 - val_loss: 24868.6152\n",
      "Epoch 168/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 24364.9160 - val_loss: 24829.2246\n",
      "Epoch 169/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 24327.4316 - val_loss: 24790.7812\n",
      "Epoch 170/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 24290.8496 - val_loss: 24753.0000\n",
      "Epoch 171/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 24254.5938 - val_loss: 24715.5312\n",
      "Epoch 172/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 24218.7402 - val_loss: 24678.4863\n",
      "Epoch 173/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 24183.5293 - val_loss: 24641.9570\n",
      "Epoch 174/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 24148.7754 - val_loss: 24605.9238\n",
      "Epoch 175/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 24114.5039 - val_loss: 24570.3730\n",
      "Epoch 176/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 24080.6133 - val_loss: 24535.2305\n",
      "Epoch 177/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 24047.3750 - val_loss: 24500.6797\n",
      "Epoch 178/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 24014.4551 - val_loss: 24466.5254\n",
      "Epoch 179/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 23982.5684 - val_loss: 24433.6055\n",
      "Epoch 180/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 23951.2988 - val_loss: 24400.8945\n",
      "Epoch 181/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 23920.1523 - val_loss: 24368.2793\n",
      "Epoch 182/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 23889.1934 - val_loss: 24335.9219\n",
      "Epoch 183/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 23858.5977 - val_loss: 24304.0469\n",
      "Epoch 184/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 23828.4082 - val_loss: 24272.4961\n",
      "Epoch 185/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 23798.3945 - val_loss: 24240.9883\n",
      "Epoch 186/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 23768.7988 - val_loss: 24210.0762\n",
      "Epoch 187/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 23739.4238 - val_loss: 24179.1777\n",
      "Epoch 188/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 23710.3730 - val_loss: 24148.7617\n",
      "Epoch 189/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 23681.7910 - val_loss: 24118.8906\n",
      "Epoch 190/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 23653.6133 - val_loss: 24089.3848\n",
      "Epoch 191/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 23625.6094 - val_loss: 24060.0547\n",
      "Epoch 192/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 23598.0293 - val_loss: 24031.2402\n",
      "Epoch 193/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 23570.7852 - val_loss: 24002.5703\n",
      "Epoch 194/500\n",
      "1172/1172 [==============================] - 6s 6ms/step - loss: 23543.8262 - val_loss: 23974.3281\n",
      "Epoch 195/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 23517.0254 - val_loss: 23946.2715\n",
      "Epoch 196/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 23490.6836 - val_loss: 23918.8301\n",
      "Epoch 197/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 23464.7012 - val_loss: 23891.6367\n",
      "Epoch 198/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 23439.0254 - val_loss: 23864.7266\n",
      "Epoch 199/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 23413.8887 - val_loss: 23838.6504\n",
      "Epoch 200/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 23389.3223 - val_loss: 23812.7852\n",
      "Epoch 201/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 23365.0723 - val_loss: 23787.2988\n",
      "Epoch 202/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 23341.1504 - val_loss: 23762.1465\n",
      "Epoch 203/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 23317.3809 - val_loss: 23736.9707\n",
      "Epoch 204/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 23293.9727 - val_loss: 23712.4551\n",
      "Epoch 205/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 23271.0684 - val_loss: 23688.4121\n",
      "Epoch 206/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 23248.5020 - val_loss: 23664.7246\n",
      "Epoch 207/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 23226.2695 - val_loss: 23641.2676\n",
      "Epoch 208/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 23204.5625 - val_loss: 23618.5195\n",
      "Epoch 209/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 23183.2578 - val_loss: 23596.1445\n",
      "Epoch 210/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 23162.3320 - val_loss: 23573.8535\n",
      "Epoch 211/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 23141.6328 - val_loss: 23551.8965\n",
      "Epoch 212/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 23121.2910 - val_loss: 23530.2598\n",
      "Epoch 213/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 23101.3516 - val_loss: 23509.1777\n",
      "Epoch 214/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 23081.9023 - val_loss: 23488.3086\n",
      "Epoch 215/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 23062.6191 - val_loss: 23467.6484\n",
      "Epoch 216/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 23043.5820 - val_loss: 23447.2832\n",
      "Epoch 217/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 23024.9434 - val_loss: 23427.3281\n",
      "Epoch 218/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 23006.6543 - val_loss: 23407.7617\n",
      "Epoch 219/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22988.8477 - val_loss: 23388.7715\n",
      "Epoch 220/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22971.4570 - val_loss: 23370.3340\n",
      "Epoch 221/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22954.6309 - val_loss: 23352.3770\n",
      "Epoch 222/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22937.9492 - val_loss: 23334.5566\n",
      "Epoch 223/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22921.5566 - val_loss: 23317.2578\n",
      "Epoch 224/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22905.5918 - val_loss: 23300.2656\n",
      "Epoch 225/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22889.8262 - val_loss: 23283.4980\n",
      "Epoch 226/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22874.3516 - val_loss: 23267.3906\n",
      "Epoch 227/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22859.4395 - val_loss: 23251.8105\n",
      "Epoch 228/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22844.8926 - val_loss: 23236.5527\n",
      "Epoch 229/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22830.7402 - val_loss: 23221.7656\n",
      "Epoch 230/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22816.8027 - val_loss: 23207.1309\n",
      "Epoch 231/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22803.3398 - val_loss: 23193.1992\n",
      "Epoch 232/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22790.2910 - val_loss: 23179.6523\n",
      "Epoch 233/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22777.9863 - val_loss: 23166.8809\n",
      "Epoch 234/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22766.1055 - val_loss: 23154.3379\n",
      "Epoch 235/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22754.5352 - val_loss: 23142.1074\n",
      "Epoch 236/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22743.2910 - val_loss: 23130.2305\n",
      "Epoch 237/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22732.3164 - val_loss: 23118.5117\n",
      "Epoch 238/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22721.6992 - val_loss: 23107.3086\n",
      "Epoch 239/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22711.4609 - val_loss: 23096.5938\n",
      "Epoch 240/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22701.7285 - val_loss: 23086.4473\n",
      "Epoch 241/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22692.2695 - val_loss: 23076.6406\n",
      "Epoch 242/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22683.1777 - val_loss: 23067.2637\n",
      "Epoch 243/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22674.4668 - val_loss: 23058.2207\n",
      "Epoch 244/500\n",
      "1172/1172 [==============================] - 8s 7ms/step - loss: 22666.0918 - val_loss: 23049.5527\n",
      "Epoch 245/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22658.0508 - val_loss: 23041.2441\n",
      "Epoch 246/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22650.2246 - val_loss: 23033.0176\n",
      "Epoch 247/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22642.7461 - val_loss: 23025.2207\n",
      "Epoch 248/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22635.5527 - val_loss: 23017.6621\n",
      "Epoch 249/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22628.7402 - val_loss: 23010.6172\n",
      "Epoch 250/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22622.3008 - val_loss: 23003.8340\n",
      "Epoch 251/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22616.0859 - val_loss: 22997.3008\n",
      "Epoch 252/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22610.1172 - val_loss: 22990.9668\n",
      "Epoch 253/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22604.3223 - val_loss: 22984.9023\n",
      "Epoch 254/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22598.7754 - val_loss: 22979.1465\n",
      "Epoch 255/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22593.5195 - val_loss: 22973.6445\n",
      "Epoch 256/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22588.6445 - val_loss: 22968.5293\n",
      "Epoch 257/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22584.0215 - val_loss: 22963.5312\n",
      "Epoch 258/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22579.6836 - val_loss: 22958.7773\n",
      "Epoch 259/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22575.4688 - val_loss: 22954.2305\n",
      "Epoch 260/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22571.5684 - val_loss: 22949.9023\n",
      "Epoch 261/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22567.7441 - val_loss: 22945.6152\n",
      "Epoch 262/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22564.0234 - val_loss: 22941.5977\n",
      "Epoch 263/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22560.4961 - val_loss: 22937.8125\n",
      "Epoch 264/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22557.1016 - val_loss: 22934.1445\n",
      "Epoch 265/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22553.7715 - val_loss: 22930.6543\n",
      "Epoch 266/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22550.7793 - val_loss: 22927.4941\n",
      "Epoch 267/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22547.9062 - val_loss: 22924.4160\n",
      "Epoch 268/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22545.2031 - val_loss: 22921.4922\n",
      "Epoch 269/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22542.6172 - val_loss: 22918.7520\n",
      "Epoch 270/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22540.1680 - val_loss: 22916.2070\n",
      "Epoch 271/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22537.9199 - val_loss: 22913.8145\n",
      "Epoch 272/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22535.7852 - val_loss: 22911.5703\n",
      "Epoch 273/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22533.8105 - val_loss: 22909.4082\n",
      "Epoch 274/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22531.8730 - val_loss: 22907.3672\n",
      "Epoch 275/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22530.0918 - val_loss: 22905.4316\n",
      "Epoch 276/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22528.3184 - val_loss: 22903.6270\n",
      "Epoch 277/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22526.6816 - val_loss: 22901.9199\n",
      "Epoch 278/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22525.1406 - val_loss: 22900.3164\n",
      "Epoch 279/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22523.6895 - val_loss: 22898.8066\n",
      "Epoch 280/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22522.3809 - val_loss: 22897.4297\n",
      "Epoch 281/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22521.1250 - val_loss: 22896.0996\n",
      "Epoch 282/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22519.9336 - val_loss: 22894.8457\n",
      "Epoch 283/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22518.8164 - val_loss: 22893.6387\n",
      "Epoch 284/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22517.7324 - val_loss: 22892.5098\n",
      "Epoch 285/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22516.7383 - val_loss: 22891.4160\n",
      "Epoch 286/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22515.7715 - val_loss: 22890.3574\n",
      "Epoch 287/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22514.8398 - val_loss: 22889.3398\n",
      "Epoch 288/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22513.9746 - val_loss: 22888.3613\n",
      "Epoch 289/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22513.1738 - val_loss: 22887.4551\n",
      "Epoch 290/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22512.4023 - val_loss: 22886.6211\n",
      "Epoch 291/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22511.7500 - val_loss: 22885.8516\n",
      "Epoch 292/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22511.1211 - val_loss: 22885.1523\n",
      "Epoch 293/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22510.5508 - val_loss: 22884.4844\n",
      "Epoch 294/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22509.9902 - val_loss: 22883.8340\n",
      "Epoch 295/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22509.4668 - val_loss: 22883.2402\n",
      "Epoch 296/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22508.9824 - val_loss: 22882.6836\n",
      "Epoch 297/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22508.4688 - val_loss: 22882.1426\n",
      "Epoch 298/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22508.0312 - val_loss: 22881.6152\n",
      "Epoch 299/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22507.5781 - val_loss: 22881.1270\n",
      "Epoch 300/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22507.1523 - val_loss: 22880.6680\n",
      "Epoch 301/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22506.7344 - val_loss: 22880.2324\n",
      "Epoch 302/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22506.3984 - val_loss: 22879.9160\n",
      "Epoch 303/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22506.1465 - val_loss: 22879.6660\n",
      "Epoch 304/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22505.9160 - val_loss: 22879.4258\n",
      "Epoch 305/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22505.7383 - val_loss: 22879.2168\n",
      "Epoch 306/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22505.5410 - val_loss: 22879.0020\n",
      "Epoch 307/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22505.3711 - val_loss: 22878.8281\n",
      "Epoch 308/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22505.2051 - val_loss: 22878.6309\n",
      "Epoch 309/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22505.0664 - val_loss: 22878.4492\n",
      "Epoch 310/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22504.9102 - val_loss: 22878.2969\n",
      "Epoch 311/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22504.7930 - val_loss: 22878.1309\n",
      "Epoch 312/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22504.6719 - val_loss: 22877.9961\n",
      "Epoch 313/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22504.5469 - val_loss: 22877.8652\n",
      "Epoch 314/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22504.4531 - val_loss: 22877.7461\n",
      "Epoch 315/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22504.3516 - val_loss: 22877.6387\n",
      "Epoch 316/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22504.2617 - val_loss: 22877.5059\n",
      "Epoch 317/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22504.1777 - val_loss: 22877.4238\n",
      "Epoch 318/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22504.1152 - val_loss: 22877.3398\n",
      "Epoch 319/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22504.0312 - val_loss: 22877.2480\n",
      "Epoch 320/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22503.9590 - val_loss: 22877.1680\n",
      "Epoch 321/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22503.8867 - val_loss: 22877.0723\n",
      "Epoch 322/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22503.8340 - val_loss: 22877.0020\n",
      "Epoch 323/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22503.7695 - val_loss: 22876.9238\n",
      "Epoch 324/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22503.7129 - val_loss: 22876.8398\n",
      "Epoch 325/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22503.6660 - val_loss: 22876.7734\n",
      "Epoch 326/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22503.6211 - val_loss: 22876.7227\n",
      "Epoch 327/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22503.5859 - val_loss: 22876.6602\n",
      "Epoch 328/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22503.5566 - val_loss: 22876.6016\n",
      "Epoch 329/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22503.5137 - val_loss: 22876.5410\n",
      "Epoch 330/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22503.4609 - val_loss: 22876.4980\n",
      "Epoch 331/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22503.4082 - val_loss: 22876.4492\n",
      "Epoch 332/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22503.3906 - val_loss: 22876.4141\n",
      "Epoch 333/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22503.3535 - val_loss: 22876.3535\n",
      "Epoch 334/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22503.2988 - val_loss: 22876.3203\n",
      "Epoch 335/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22503.2871 - val_loss: 22876.2695\n",
      "Epoch 336/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22503.2324 - val_loss: 22876.2285\n",
      "Epoch 337/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22503.2246 - val_loss: 22876.1934\n",
      "Epoch 338/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22503.1973 - val_loss: 22876.1543\n",
      "Epoch 339/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22503.1758 - val_loss: 22876.1270\n",
      "Epoch 340/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22503.1777 - val_loss: 22876.0977\n",
      "Epoch 341/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22503.1191 - val_loss: 22876.0527\n",
      "Epoch 342/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22503.0898 - val_loss: 22876.0273\n",
      "Epoch 343/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22503.0684 - val_loss: 22875.9961\n",
      "Epoch 344/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22503.0547 - val_loss: 22875.9570\n",
      "Epoch 345/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22503.0410 - val_loss: 22875.9414\n",
      "Epoch 346/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22503.0254 - val_loss: 22875.9199\n",
      "Epoch 347/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.9883 - val_loss: 22875.8965\n",
      "Epoch 348/500\n",
      "1172/1172 [==============================] - 5s 4ms/step - loss: 22502.9785 - val_loss: 22875.8828\n",
      "Epoch 349/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22502.9785 - val_loss: 22875.8496\n",
      "Epoch 350/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22502.9668 - val_loss: 22875.8223\n",
      "Epoch 351/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22502.9531 - val_loss: 22875.8184\n",
      "Epoch 352/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22502.9355 - val_loss: 22875.8047\n",
      "Epoch 353/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22502.9102 - val_loss: 22875.7852\n",
      "Epoch 354/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.9023 - val_loss: 22875.7656\n",
      "Epoch 355/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22502.8867 - val_loss: 22875.7383\n",
      "Epoch 356/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22502.8770 - val_loss: 22875.7207\n",
      "Epoch 357/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.8594 - val_loss: 22875.7129\n",
      "Epoch 358/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.8594 - val_loss: 22875.6875\n",
      "Epoch 359/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.8359 - val_loss: 22875.6855\n",
      "Epoch 360/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.8242 - val_loss: 22875.6699\n",
      "Epoch 361/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.8262 - val_loss: 22875.6445\n",
      "Epoch 362/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.8398 - val_loss: 22875.6348\n",
      "Epoch 363/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.8262 - val_loss: 22875.6230\n",
      "Epoch 364/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.8223 - val_loss: 22875.6211\n",
      "Epoch 365/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7988 - val_loss: 22875.6191\n",
      "Epoch 366/500\n",
      "1172/1172 [==============================] - 8s 6ms/step - loss: 22502.8066 - val_loss: 22875.5938\n",
      "Epoch 367/500\n",
      "1172/1172 [==============================] - 10s 8ms/step - loss: 22502.8125 - val_loss: 22875.5938\n",
      "Epoch 368/500\n",
      "1172/1172 [==============================] - 8s 7ms/step - loss: 22502.7852 - val_loss: 22875.5859\n",
      "Epoch 369/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7539 - val_loss: 22875.5684\n",
      "Epoch 370/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7812 - val_loss: 22875.5586\n",
      "Epoch 371/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7871 - val_loss: 22875.5508\n",
      "Epoch 372/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7793 - val_loss: 22875.5508\n",
      "Epoch 373/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7812 - val_loss: 22875.5508\n",
      "Epoch 374/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7695 - val_loss: 22875.5391\n",
      "Epoch 375/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7480 - val_loss: 22875.5293\n",
      "Epoch 376/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7871 - val_loss: 22875.5449\n",
      "Epoch 377/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7734 - val_loss: 22875.5234\n",
      "Epoch 378/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7676 - val_loss: 22875.5293\n",
      "Epoch 379/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7656 - val_loss: 22875.5234\n",
      "Epoch 380/500\n",
      "1172/1172 [==============================] - 5s 5ms/step - loss: 22502.7324 - val_loss: 22875.5273\n",
      "Epoch 381/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7676 - val_loss: 22875.5117\n",
      "Epoch 382/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7598 - val_loss: 22875.5117\n",
      "Epoch 383/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7598 - val_loss: 22875.4922\n",
      "Epoch 384/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7559 - val_loss: 22875.4902\n",
      "Epoch 385/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7363 - val_loss: 22875.5059\n",
      "Epoch 386/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7695 - val_loss: 22875.4941\n",
      "Epoch 387/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7480 - val_loss: 22875.4863\n",
      "Epoch 388/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7520 - val_loss: 22875.4824\n",
      "Epoch 389/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7500 - val_loss: 22875.4844\n",
      "Epoch 390/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7520 - val_loss: 22875.4883\n",
      "Epoch 391/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7598 - val_loss: 22875.4883\n",
      "Epoch 392/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7285 - val_loss: 22875.4883\n",
      "Epoch 393/500\n",
      "1172/1172 [==============================] - 6s 6ms/step - loss: 22502.7617 - val_loss: 22875.4863\n",
      "Epoch 394/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7324 - val_loss: 22875.4883\n",
      "Epoch 395/500\n",
      "1172/1172 [==============================] - 6s 6ms/step - loss: 22502.7461 - val_loss: 22875.4863\n",
      "Epoch 396/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7324 - val_loss: 22875.4824\n",
      "Epoch 397/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7422 - val_loss: 22875.4727\n",
      "Epoch 398/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7422 - val_loss: 22875.4727\n",
      "Epoch 399/500\n",
      "1172/1172 [==============================] - 8s 6ms/step - loss: 22502.7461 - val_loss: 22875.4668\n",
      "Epoch 400/500\n",
      "1172/1172 [==============================] - 9s 7ms/step - loss: 22502.7578 - val_loss: 22875.4629\n",
      "Epoch 401/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7539 - val_loss: 22875.4707\n",
      "Epoch 402/500\n",
      "1172/1172 [==============================] - 8s 6ms/step - loss: 22502.7402 - val_loss: 22875.4648\n",
      "Epoch 403/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7480 - val_loss: 22875.4668\n",
      "Epoch 404/500\n",
      "1172/1172 [==============================] - 8s 7ms/step - loss: 22502.7422 - val_loss: 22875.4609\n",
      "Epoch 405/500\n",
      "1172/1172 [==============================] - 8s 7ms/step - loss: 22502.7617 - val_loss: 22875.4609\n",
      "Epoch 406/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7617 - val_loss: 22875.4551\n",
      "Epoch 407/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7402 - val_loss: 22875.4590\n",
      "Epoch 408/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7598 - val_loss: 22875.4551\n",
      "Epoch 409/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7266 - val_loss: 22875.4668\n",
      "Epoch 410/500\n",
      "1172/1172 [==============================] - 8s 7ms/step - loss: 22502.7402 - val_loss: 22875.4590\n",
      "Epoch 411/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7441 - val_loss: 22875.4609\n",
      "Epoch 412/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7441 - val_loss: 22875.4609\n",
      "Epoch 413/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7441 - val_loss: 22875.4668\n",
      "Epoch 414/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7539 - val_loss: 22875.4648\n",
      "Epoch 415/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7422 - val_loss: 22875.4629\n",
      "Epoch 416/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7461 - val_loss: 22875.4551\n",
      "Epoch 417/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7461 - val_loss: 22875.4531\n",
      "Epoch 418/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7324 - val_loss: 22875.4551\n",
      "Epoch 419/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7227 - val_loss: 22875.4531\n",
      "Epoch 420/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7500 - val_loss: 22875.4512\n",
      "Epoch 421/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7520 - val_loss: 22875.4512\n",
      "Epoch 422/500\n",
      "1172/1172 [==============================] - 8s 7ms/step - loss: 22502.7324 - val_loss: 22875.4375\n",
      "Epoch 423/500\n",
      "1172/1172 [==============================] - 8s 7ms/step - loss: 22502.7617 - val_loss: 22875.4336\n",
      "Epoch 424/500\n",
      "1172/1172 [==============================] - 8s 7ms/step - loss: 22502.7148 - val_loss: 22875.4316\n",
      "Epoch 425/500\n",
      "1172/1172 [==============================] - 8s 7ms/step - loss: 22502.7520 - val_loss: 22875.4336\n",
      "Epoch 426/500\n",
      "1172/1172 [==============================] - 10s 9ms/step - loss: 22502.7598 - val_loss: 22875.4395\n",
      "Epoch 427/500\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 22502.7422 - val_loss: 22875.4551\n",
      "Epoch 428/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7441 - val_loss: 22875.4375\n",
      "Epoch 429/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7461 - val_loss: 22875.4336\n",
      "Epoch 430/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7383 - val_loss: 22875.4336\n",
      "Epoch 431/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7344 - val_loss: 22875.4336\n",
      "Epoch 432/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7695 - val_loss: 22875.4277\n",
      "Epoch 433/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7461 - val_loss: 22875.4297\n",
      "Epoch 434/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7324 - val_loss: 22875.4316\n",
      "Epoch 435/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7461 - val_loss: 22875.4258\n",
      "Epoch 436/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7441 - val_loss: 22875.4297\n",
      "Epoch 437/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7461 - val_loss: 22875.4258\n",
      "Epoch 438/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7500 - val_loss: 22875.4316\n",
      "Epoch 439/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7461 - val_loss: 22875.4258\n",
      "Epoch 440/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7539 - val_loss: 22875.4258\n",
      "Epoch 441/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7324 - val_loss: 22875.4316\n",
      "Epoch 442/500\n",
      "1172/1172 [==============================] - 8s 6ms/step - loss: 22502.7422 - val_loss: 22875.4336\n",
      "Epoch 443/500\n",
      "1172/1172 [==============================] - 8s 7ms/step - loss: 22502.7363 - val_loss: 22875.4316\n",
      "Epoch 444/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7520 - val_loss: 22875.4258\n",
      "Epoch 445/500\n",
      "1172/1172 [==============================] - 8s 7ms/step - loss: 22502.7383 - val_loss: 22875.4277\n",
      "Epoch 446/500\n",
      "1172/1172 [==============================] - 8s 7ms/step - loss: 22502.7461 - val_loss: 22875.4258\n",
      "Epoch 447/500\n",
      "1172/1172 [==============================] - 9s 7ms/step - loss: 22502.7559 - val_loss: 22875.4258\n",
      "Epoch 448/500\n",
      "1172/1172 [==============================] - 8s 7ms/step - loss: 22502.7598 - val_loss: 22875.4316\n",
      "Epoch 449/500\n",
      "1172/1172 [==============================] - 8s 7ms/step - loss: 22502.7344 - val_loss: 22875.4297\n",
      "Epoch 450/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7480 - val_loss: 22875.4238\n",
      "Epoch 451/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7207 - val_loss: 22875.4297\n",
      "Epoch 452/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7402 - val_loss: 22875.4316\n",
      "Epoch 453/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7246 - val_loss: 22875.4316\n",
      "Epoch 454/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7168 - val_loss: 22875.4297\n",
      "Epoch 455/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7500 - val_loss: 22875.4297\n",
      "Epoch 456/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7500 - val_loss: 22875.4297\n",
      "Epoch 457/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7598 - val_loss: 22875.4277\n",
      "Epoch 458/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7305 - val_loss: 22875.4355\n",
      "Epoch 459/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7461 - val_loss: 22875.4316\n",
      "Epoch 460/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7344 - val_loss: 22875.4297\n",
      "Epoch 461/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7480 - val_loss: 22875.4375\n",
      "Epoch 462/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7559 - val_loss: 22875.4297\n",
      "Epoch 463/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7266 - val_loss: 22875.4297\n",
      "Epoch 464/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7441 - val_loss: 22875.4277\n",
      "Epoch 465/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7461 - val_loss: 22875.4297\n",
      "Epoch 466/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7617 - val_loss: 22875.4316\n",
      "Epoch 467/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7461 - val_loss: 22875.4316\n",
      "Epoch 468/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7461 - val_loss: 22875.4297\n",
      "Epoch 469/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7441 - val_loss: 22875.4258\n",
      "Epoch 470/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7461 - val_loss: 22875.4316\n",
      "Epoch 471/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7422 - val_loss: 22875.4238\n",
      "Epoch 472/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7305 - val_loss: 22875.4316\n",
      "Epoch 473/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7637 - val_loss: 22875.4316\n",
      "Epoch 474/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7383 - val_loss: 22875.4316\n",
      "Epoch 475/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7520 - val_loss: 22875.4355\n",
      "Epoch 476/500\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 22502.7402 - val_loss: 22875.4277\n",
      "Epoch 477/500\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 22502.7148 - val_loss: 22875.4316\n",
      "Epoch 478/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7305 - val_loss: 22875.4316\n",
      "Epoch 479/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7598 - val_loss: 22875.4258\n",
      "Epoch 480/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7422 - val_loss: 22875.4297\n",
      "Epoch 481/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7559 - val_loss: 22875.4355\n",
      "Epoch 482/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7324 - val_loss: 22875.4316\n",
      "Epoch 483/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7461 - val_loss: 22875.4277\n",
      "Epoch 484/500\n",
      "1172/1172 [==============================] - 8s 7ms/step - loss: 22502.7461 - val_loss: 22875.4258\n",
      "Epoch 485/500\n",
      "1172/1172 [==============================] - 8s 6ms/step - loss: 22502.7539 - val_loss: 22875.4277\n",
      "Epoch 486/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7441 - val_loss: 22875.4219\n",
      "Epoch 487/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7520 - val_loss: 22875.4258\n",
      "Epoch 488/500\n",
      "1172/1172 [==============================] - 9s 7ms/step - loss: 22502.7324 - val_loss: 22875.4297\n",
      "Epoch 489/500\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 22502.7461 - val_loss: 22875.4316\n",
      "Epoch 490/500\n",
      "1172/1172 [==============================] - 9s 8ms/step - loss: 22502.7324 - val_loss: 22875.4297\n",
      "Epoch 491/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7520 - val_loss: 22875.4297\n",
      "Epoch 492/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7344 - val_loss: 22875.4277\n",
      "Epoch 493/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7324 - val_loss: 22875.4316\n",
      "Epoch 494/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7441 - val_loss: 22875.4277\n",
      "Epoch 495/500\n",
      "1172/1172 [==============================] - 6s 5ms/step - loss: 22502.7520 - val_loss: 22875.4238\n",
      "Epoch 496/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7441 - val_loss: 22875.4219\n",
      "Epoch 497/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7500 - val_loss: 22875.4258\n",
      "Epoch 498/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7148 - val_loss: 22875.4219\n",
      "Epoch 499/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7324 - val_loss: 22875.4277\n",
      "Epoch 500/500\n",
      "1172/1172 [==============================] - 7s 6ms/step - loss: 22502.7578 - val_loss: 22875.4277\n"
     ]
    }
   ],
   "source": [
    "# Compile and train the model\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "history = model.fit([X_pasar_train, X_train_train], y_train, epochs=500, batch_size=32, validation_data=([X_pasar_test, X_train_test], y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Model Evaluation**: The trained model is evaluated on the test data using the mean absolute error as the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503/503 [==============================] - 1s 2ms/step - loss: 22875.4277\n",
      "Test Loss: 22875.427734375\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss = model.evaluate([X_pasar_test, X_train_test], y_test)\n",
    "\n",
    "# Print the evaluation result\n",
    "print(\"Test Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the training and validation loss values from the history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot the training and validation loss\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "plt.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'g-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **User Input and Price Prediction**: User inputs for market location, geographical coordinates, category, and commodity are provided. The inputs are preprocessed, encoded, and combined with numeric features. The model predicts the grocery price based on the user inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinsi = \"JAWA TENGAH\"\n",
    "kabupaten = \"KOTA SEMARANG\"\n",
    "latitude = -7.058040\n",
    "longitude = 110.445680\n",
    "category = \"meat, fish and eggs\"\n",
    "commodity = \"Eggs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess user input\n",
    "user_pasar = tokenizer.texts_to_sequences([kabupaten])  # Tokenize 'pasar'\n",
    "user_pasar = pad_sequences(user_pasar, maxlen=max_length)  # Pad the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wilbert\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform the categorical features\n",
    "user_provinsi_encoded = onehot_encoder.fit_transform([[provinsi]])\n",
    "user_category_encoded = onehot_encoder.transform([[category]])\n",
    "user_commodity_encoded = onehot_encoder.transform([[commodity]])\n",
    "user_kabupaten_encoded = onehot_encoder.transform([[kabupaten]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step\n"
     ]
    }
   ],
   "source": [
    "# Combine the numeric and categorical features\n",
    "user_numeric_features = np.array([[latitude, longitude]])  # Reshape to (1, 2)\n",
    "user_features = np.concatenate((user_numeric_features, user_provinsi_encoded, user_kabupaten_encoded, user_category_encoded, user_commodity_encoded), axis=1)\n",
    "\n",
    "# Take only the first three elements from user_features\n",
    "user_features = user_features[:, :6]\n",
    "\n",
    "# Make predictions\n",
    "predicted_price = model.predict([user_pasar, user_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the predicted prices in ascending order\n",
    "sorted_indices = np.argsort(predicted_price.flatten())\n",
    "sorted_prices = predicted_price[sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top five market locations with the lowest predicted prices\n",
    "recommended_locations = dataset.iloc[sorted_indices[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Data Length: 14\n"
     ]
    }
   ],
   "source": [
    "# Filter data based on user input\n",
    "filtered_data = dataset[(dataset['provinsi'] == provinsi) & (dataset['kabupaten'] == kabupaten) & (dataset['category'] == category) & (dataset['commodity'] == commodity)].copy()\n",
    "print(\"Filtered Data Length:\", len(filtered_data))  # Add this line to check the length of filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame to store recommended locations with distances\n",
    "recommended_locations = pd.DataFrame(columns=['pasar', 'price', 'latitude', 'longitude', 'distance'])\n",
    "\n",
    "# Create a set to store the visited markets\n",
    "visited_markets = set()\n",
    "\n",
    "# Calculate distances and add rows to the recommended_locations DataFrame\n",
    "for i, row in filtered_data.iterrows():\n",
    "    market = row['pasar']\n",
    "    if market not in visited_markets:\n",
    "        distance = geodesic((latitude, longitude), (row['latitude'], row['longitude'])).km\n",
    "        recommended_locations.loc[i] = [market, row['price'], row['latitude'], row['longitude'], distance]\n",
    "        visited_markets.add(market)\n",
    "\n",
    "        # Break the loop if 5 unique markets have been added\n",
    "        if len(recommended_locations) == 5:\n",
    "            break\n",
    "\n",
    "# Sort the recommendations by price and distance\n",
    "sorted_recommendations = recommended_locations.sort_values(['price', 'distance'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Market Location Recommendations**: Based on the user inputs, the code filters the dataset to find matching market locations. It calculates the distances between the user's location and each market location using the geodesic function from the geopy library. The code recommends the top five market locations with the lowest prices and closest distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input: \n",
      "JAWA TENGAH\n",
      "KOTA SEMARANG\n",
      "-7.05804\n",
      "110.44568\n",
      "meat, fish and eggs\n",
      "Eggs\n",
      "\n",
      "Top 5 Recommended Market Locations:\n",
      "Market: Pasar Johar\n",
      "Price: 19944.44\n",
      "Distance: 8.713057235562655 km\n",
      "\n",
      "Market: Pasar Peterongan\n",
      "Price: 20888.89\n",
      "Distance: 6.235409379814343 km\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the recommendations\n",
    "print(\"User input: \")\n",
    "print(provinsi)\n",
    "print(kabupaten)\n",
    "print(latitude) \n",
    "print(longitude)\n",
    "print(category)\n",
    "print(commodity)\n",
    "print()\n",
    "\n",
    "print(\"Top 5 Recommended Market Locations:\")\n",
    "for i, row in sorted_recommendations.head(5).iterrows():\n",
    "    print(f\"Market: {row['pasar']}\")\n",
    "    print(f\"Price: {row['price']}\")\n",
    "    print(f\"Distance: {row['distance']} km\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **Save The Model**: Save the model using h5 and pickle format. The model will saved as \"Store_Recommendation.h5\" and \"model.pickle\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Store_Recommendation.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "To run the code, follow these steps:\n",
    "\n",
    "1. Install the necessary dependencies mentioned in the Prerequisites section.\n",
    "\n",
    "2. Make sure you have the 'Data_Harga1.csv' dataset in the same directory as the code file.\n",
    "\n",
    "3. Run the code file using a Python interpreter.\n",
    "\n",
    "4. The code will prompt for user inputs such as the province, district, latitude, longitude, category, and commodity. Provide the required information and press Enter.\n",
    "\n",
    "5. The code will display the predicted grocery price based on the user inputs.\n",
    "\n",
    "6. It will also show the top five recommended market locations based on price and distance.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The Smart Grocery Price Prediction code demonstrates the use of machine learning techniques to predict grocery prices and provide recommendations for market locations. It utilizes text and numeric inputs, employs an LSTM-based model architecture, and incorporates geographic distance calculations. By running this code, users can predict grocery prices and find nearby market locations with the best prices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
